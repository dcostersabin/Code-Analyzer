import timeit
from Python.ModuleChecker import ModuleChecker
import os
import io
import pandas as pd
import tracemalloc
from os import path
import shutil
import matplotlib.pyplot as plt

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
__REPEAT__ = 5
__NUMBER__ = 1
__SAVE_PATH__ = BASE_DIR


class PythonBenchmark:
    """
    Benchmarking python codes
    Completeness
    Correctness
    Time Complexity
    Peak Memory Usages
    Detailed Function Profiling
    """

    def __init__(self, url, expected_output, params):
        """
        :param url:  Url To Be Cloned or Github repository HTTPS Link SSH Is Not Supported
        :param expected_output: Actual output generated by the algorithm
        :param params: Parameters For Function Testing
        """
        self.url = url
        self.repo_validity = ModuleChecker(self.url).clone_check()
        self.benchmark_score = {"complete": False, "correctness": False, "time": None, "memory": None,
                                "detailed_profiling": None}
        self.target_output = expected_output
        self.calculated_output = None
        self.params = params

    def start(self):
        """
        :return: False if the given repository link is not valid otherwise returns the defined complexities score
        """
        if self.repo_validity:
            self.__completeness__()
            self.remove_temp()
            return self.benchmark_score
        else:
            self.remove_temp()
            return False

    def __time_complexity__(self):
        """
        Measures the time taken by the code to execute for __REPEAT__ times
        :return: None
        """
        params = self.params
        setup_code = "from temp.test import Testing"
        stmt_code = "Testing.setup(" + str(params) + ")"
        time = timeit.repeat(stmt=stmt_code, setup=setup_code, repeat=__REPEAT__, number=__NUMBER__)
        self.benchmark_score['time'] = time

    def __space_complexity__(self):
        """
        Measures the peak memory used by the defined function
        :return: None
        """
        from temp.test import Testing
        tracemalloc.start()
        Testing.setup(self.params)
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        self.benchmark_score['memory'] = peak / 10 ** 6

    def __completeness__(self):
        """
        Checks if the given module provides any output or not
        :return: None
        """
        from temp.test import Testing
        result = Testing.setup(self.params)
        if result is not None:
            self.benchmark_score["complete"] = True
            try:
                assert result == self.target_output
                self.benchmark_score["correctness"] = True
                self.__time_complexity__()
                self.__detailed_profiling__()
                self.__space_complexity__()
            except AssertionError:
                self.benchmark_score["correctness"] = False

    def __test__(self):
        """
        Testing the downloaded module
        :return: None
        """
        from temp.test import Testing
        a = Testing.setup(self.params)

    def __detailed_profiling__(self):
        """
        Using cProfile to aggregate the resources used by the function
        :return: None
        """
        import cProfile, pstats
        profiler = cProfile.Profile()
        profiler.enable()
        self.__test__()
        profiler.disable()
        stream = io.StringIO()
        pstats.Stats(profiler, stream=stream).sort_stats('ncalls').strip_dirs().print_stats()
        stream = stream.getvalue()
        self.__convert_to_csv__(stream)
        self.benchmark_score['detailed_profiling'] = pd.read_csv(BASE_DIR + '/temp/stat.csv')

    @staticmethod
    def __convert_to_csv__(stream):
        """
        Converting crpofile output to csv
        :param stream: IOStram for the cprofile output
        :return: None
        """
        result = 'ncalls' + stream.split('ncalls')[-1]
        result = '\n'.join([','.join(line.rstrip().split(None, 5)) for line in result.split('\n')])
        f = open(BASE_DIR + '/temp/stat'.rsplit('.')[0] + '.csv', 'w')
        f.write(result)
        f.close()

    def remove_temp(self):
        """
        Removes the temporary files after results are calculated
        :return: None
        """
        if path.exists(BASE_DIR + '/temp/'):
            shutil.rmtree(BASE_DIR + '/temp/')

    def visualize(self, params='time', save=False):
        """
        Visualize the analyzed data
        :param params: the item needed to be visualized
        :param save: save the image as png if True
        :return: None
        """
        if self.benchmark_score['complete'] is True:
            if params == 'time':
                self.__visualize_time__(save)
            elif params == 'cprofile':
                self.__visualize_cprofile__()
            elif params == 'all':
                self.__visualize_time__(save)
                self.__visualize_cprofile__(save)

    def __visualize_time__(self, save=False):
        """
        Visualize the time complexity
        :param save: saves png if True
        :return: None
        """
        plt.bar(range(1, 6), self.benchmark_score['time'])
        plt.title('Time Complexity for 5 Iterations')
        plt.ylabel('Time In Seconds')
        plt.xlabel('No Of Iterations')
        if save:
            plt.savefig(__SAVE_PATH__ + '/time.png')
        plt.show()

    def __visualize_cprofile__(self, save=False):
        """
        cProfile analysis of the algorithm
        :param save: saves png if true
        :return: None
        """
        score = pd.DataFrame(self.benchmark_score['detailed_profiling'])
        ncalls = score.iloc[:, 0]
        functions = score.iloc[:, -1]
        plt.bar(functions, ncalls)
        plt.title('No Of Functions Calls')
        plt.xticks(rotation=45, ha="right")
        for index, data in enumerate(ncalls):
            plt.text(x=index, y=data + 1, s=f"{data}", fontdict=dict(fontsize=8))
        plt.tight_layout()
        if save:
            plt.savefig(__SAVE_PATH__ + '/function_calls.png')
        plt.show()
